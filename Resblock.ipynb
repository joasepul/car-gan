{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Add, Dot, Lambda, Conv2DTranspose, Dot, Activation, Reshape, BatchNormalization, UpSampling2D, AveragePooling2D, GlobalAveragePooling2D, Multiply, LeakyReLU, Flatten, MaxPool2D \n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResBlockDown(input_shape, channel_size, channel_multiplier=1, name=None):\n",
    "    # Resblock architecture\n",
    "    # 1 BatchNorm \n",
    "    # 2 ReLU activation\n",
    "    # 3 Conv layer\n",
    "    # 4 BatchNorm\n",
    "    # 5 ReLU activation\n",
    "    # 6 Conv layer\n",
    "    # 7 Sum with input \n",
    "    \n",
    "    #FIRST BLOCK\n",
    "    #input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # BatchNorm - needs to be conditional\n",
    "    resblock = BatchNormalization()(input_layer)\n",
    "    \n",
    "    # Relu\n",
    "    resblock = Activation('relu')(resblock)\n",
    "    \n",
    "    # Convolution size 3 filter as per paper\n",
    "    # Need to spectrally normalize here somehow\n",
    "    resblock = Conv2D(channel_size * channel_multiplier, 3, padding='same')(resblock)\n",
    "    \n",
    "    #SECOND BLOCK\n",
    "    \n",
    "    # BatchNorm - needs to be conditional\n",
    "    resblock = BatchNormalization()(input_layer)\n",
    "    \n",
    "    # Relu\n",
    "    resblock = Activation('relu')(resblock)\n",
    "    \n",
    "    # Convolution size 3 filter as per paper\n",
    "    # Need to spectrally normalize here somehow\n",
    "    resblock = Conv2D(channel_size * channel_multiplier, 3, padding='same')(resblock)\n",
    "    \n",
    "    # Downsample\n",
    "    resblock = AveragePooling2D()(resblock)\n",
    "    \n",
    "    # Time for the shortcut connection!\n",
    "    \n",
    "    shortcut_identity = Conv2D(channel_size * channel_multiplier, 1, padding='same')(input_layer)\n",
    "    shortcut_identity = AveragePooling2D()(shortcut_identity)\n",
    "    \n",
    "    output_layer = Add()([shortcut_identity, resblock])\n",
    "    \n",
    "    return Model(input_layer, output_layer, name=name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResBlock(input_shape, channel_size, channel_multiplier=1, name=None):\n",
    "     # Resblock architecture\n",
    "    # 1 BatchNorm \n",
    "    # 2 ReLU activation\n",
    "    # 3 Conv layer\n",
    "    # 4 BatchNorm\n",
    "    # 5 ReLU activation\n",
    "    # 6 Conv layer\n",
    "    # 7 Sum with input \n",
    "    \n",
    "    #FIRST BLOCK\n",
    "    #input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # BatchNorm - needs to be conditional\n",
    "    resblock = BatchNormalization()(input_layer)\n",
    "    \n",
    "    # Relu\n",
    "    resblock = Activation('relu')(resblock)\n",
    "    \n",
    "    # Convolution size 3 filter as per paper\n",
    "    # Need to spectrally normalize here somehow\n",
    "    resblock = Conv2D(channel_size * channel_multiplier, 3, padding='same')(resblock)\n",
    "    \n",
    "    #SECOND BLOCK\n",
    "    \n",
    "    # BatchNorm - needs to be conditional\n",
    "    resblock = BatchNormalization()(input_layer)\n",
    "    \n",
    "    # Relu\n",
    "    resblock = Activation('relu')(resblock)\n",
    "    \n",
    "    # Convolution size 3 filter as per paper\n",
    "    # Need to spectrally normalize here somehow\n",
    "    resblock = Conv2D(channel_size * channel_multiplier, 3, padding='same')(resblock)\n",
    "    \n",
    "    \n",
    "    # Time for the shortcut connection!\n",
    "    \n",
    "    shortcut_identity = Conv2D(channel_size * channel_multiplier, 1, padding='same')(input_layer)\n",
    "    \n",
    "    output_layer = Add()([shortcut_identity, resblock])\n",
    "    \n",
    "    return Model(input_layer, output_layer, name=name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SelfAttentionBlock(input_shape, name=None):\n",
    "    # f = conv\n",
    "    channels = input_shape[-1]\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    f = Conv2D(channels // 8, 1, padding='same')(input_layer)\n",
    "    # f = maxpooling\n",
    "    f = MaxPool2D(pool_size=2, strides=2, padding='same')(f)\n",
    "    \n",
    "    g = Conv2D(channels // 8, 1, padding='same')(input_layer)\n",
    "    \n",
    "    h = Conv2D(channels // 2, 1, padding='same')(input_layer)\n",
    "    h = MaxPool2D(pool_size=2, strides=2, padding='same')(h)\n",
    "    \n",
    "    \n",
    "    g = Lambda(lambda input1: K.reshape(input1, shape=[-1,1]))(g)\n",
    "    f = Lambda(lambda input1: K.reshape(input1, shape=[-1,1]))(f)\n",
    "    s = Dot(-1)([g, f])\n",
    "    beta = Activation('softmax')(s)\n",
    "\n",
    "    h = Lambda(lambda input1: K.reshape(input1, shape=[-1,1]))(h)\n",
    "    o = Dot(-1)([beta, h])\n",
    "    \n",
    "    gamma = Conv2D(channels, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal')(input_layer)\n",
    "    #gamma = Reshape((-1, channels // 2))(g)\n",
    "    #print(gamma.shape)\n",
    "    a, x, y ,z = input_layer.shape\n",
    "    #o = K.reshape(o, shape=[x,y,z, channels // 2])\n",
    "    o = Lambda(lambda input1: K.reshape(input1, shape=[x,y,z, channels // 2]))(o)\n",
    "    o = Conv2D(channels, kernel_size=1, strides=1)(o)\n",
    "  \n",
    "    Wz_yi = Multiply()([gamma, o])\n",
    "    output_layer = Add()([Wz_yi, input_layer])\n",
    "    \n",
    "    return Model(input_layer, output_layer, name=name)\n",
    "    \n",
    "    # g = conv\n",
    "    # h = conv\n",
    "    # h = maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Discriminator_resblock_down_1_155/add_243/add:0\", shape=(?, 64, 64, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-ccca7eb49760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mselfattentionblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfAttentionBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Discriminator_selfattentionblock'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselfattentionblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Non local block should be here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "class GlobalSumPooling2D(_GlobalPooling2D):\n",
    "    \"\"\"Global sum pooling operation for spatial data.\n",
    "    # Arguments\n",
    "        data_format: A string,\n",
    "            one of `channels_last` (default) or `channels_first`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `channels_last` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `channels_first`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, rows, cols, channels)`\n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, rows, cols)`\n",
    "    # Output shape\n",
    "        2D tensor with shape:\n",
    "        `(batch_size, channels)`\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return K.sum(inputs, axis=[1, 2])\n",
    "        else:\n",
    "            return K.sum(inputs, axis=[2, 3])\n",
    "\n",
    "# Discriminator test\n",
    "model_input = Input(shape=(128,128,3))\n",
    "resblockdown1 = ResBlockDown(input_shape=(128,128,3),channel_size=1, channel_multiplier=64, name='Discriminator_resblock_down_1')\n",
    "h = resblockdown1(model_input)\n",
    "print(h)\n",
    "selfattentionblock = SelfAttentionBlock(input_shape=(64,64,64), name='Discriminator_selfattentionblock')\n",
    "h = selfattentionblock()(h)\n",
    "print(h)\n",
    "# Non local block should be here\n",
    "resblockdown2 = ResBlockDown(input_shape=(64,64,64),channel_size=2, channel_multiplier=64, name='Discriminator_resblock_down_2')\n",
    "h = resblockdown2(h)\n",
    "print(h)\n",
    "resblockdown4 = ResBlockDown(input_shape=(32,32,128),channel_size=4, channel_multiplier=64, name='Discriminator_resblock_down_4')\n",
    "h = resblockdown4(h)\n",
    "print(h)\n",
    "resblockdown8 = ResBlockDown(input_shape=(16,16,256),channel_size=8, channel_multiplier=64, name='Discriminator_resblock_down_8')\n",
    "h = resblockdown8(h)\n",
    "print(h)\n",
    "resblockdown16 = ResBlockDown(input_shape=(8,8,512),channel_size=16, channel_multiplier=64, name='Discriminator_resblock_down_16')\n",
    "h = resblockdown16(h)\n",
    "print(h)\n",
    "resblock16 = ResBlock(input_shape=(4,4,1024),channel_size=16, channel_multiplier=64, name='Discriminator_resblock_16')\n",
    "h = resblock16(h)\n",
    "print(h)\n",
    "h = Activation('relu')(h)\n",
    "print(h)\n",
    "h = GlobalSumPooling2D()(h)\n",
    "model_output = Dense(1)(h)\n",
    "print(h)\n",
    "# we need an embedding layer instead of dense\n",
    "model = Model(model_input, model_output, name=\"Sonofabish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
